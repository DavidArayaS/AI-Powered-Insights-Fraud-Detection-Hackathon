# ğŸ† Challenge 1: Setting Up Fabric & Storage Solutions.

## ğŸ“– Scenario  
Contoso, a global company, is implementing a **cloud-based AI-powered fraud detection system**. They need a **scalable and secure solution** to store and process financial transaction PDFs before analysis.  

Your task is to **set up Microsoft Fabric and Data Storage Solutions** to build the foundation for this system.  

---

## ğŸ¯ Your Mission  
By completing this challenge, you will:  

âœ… Set up **Microsoft Fabric Capacity**  
âœ… Create a **OneLake Lakehouse** to store financial transaction PDFs  
âœ… Upload the provided PDFs to **OneLake**  
âœ… Create & Configure a **Service Principal** for authentication  
âœ… Assign appropriate **permissions** in Fabric & Azure Storage  

> **Note:** You will not process the documents yetâ€”this is about **building the data pipeline** to store and prepare them.  

---

## ğŸš€ Step 1: Create Microsoft Fabric Capacity  
ğŸ’¡ **Why?** Microsoft Fabric provides the necessary **capacity to store and process large datasets**.  

### 1ï¸âƒ£ Create Fabric Capacity in Azure  
ğŸ”¹ Using the **Azure Portal**, create a **Fabric Capacity** resource that will support the **Lakehouse** and data pipeline.  

ğŸ”¹ **What settings should you configure?**  
   - Which **SKU** is required for this challenge?  
   - What **security features** should you enable?  

âœ… **Best Practice**: Consider setting up **RBAC (Role-Based Access Control)** to manage access.  

---

## ğŸš€ Step 2: Assign Fabric Capacity in Microsoft Fabric  
ğŸ’¡ **Why?** The Fabric workspace must be **assigned to a capacity** before you can use it.  

### 1ï¸âƒ£ Assign Fabric Capacity  
ğŸ”¹ Navigate to **Microsoft Fabric** and assign the newly created **Fabric Capacity** to a workspace.  

ğŸ”¹ **Where in the Fabric UI can you manage capacity assignments?**  

âœ… **Outcome**: Your **Fabric workspace** should now be connected to a **Fabric Capacity**.  

---

## ğŸš€ Step 3: Create a OneLake Lakehouse  
ğŸ’¡ **Why?** A **OneLake Lakehouse** is needed to store **unprocessed PDFs** before AI processing.  

### 1ï¸âƒ£ Create a Fabric Lakehouse  
ğŸ”¹ In **Microsoft Fabric**, create a **new Lakehouse** inside your workspace.  

ğŸ”¹ **What folder structure would be best for organizing financial PDFs?**  

âœ… **Best Practice**: Keep a **structured folder hierarchy** for organized data.  

---

## ğŸš€ Step 4: Upload the PDFs to OneLake  
ğŸ’¡ **Why?** The AI models need **structured and accessible financial documents**.  

### 1ï¸âƒ£ Upload PDF Files  
ğŸ”¹ Upload the provided **financial transaction PDFs** to **OneLake**.  

ğŸ”¹ **Where should you upload the files inside the Lakehouse?**  

âœ… **Best Practice**: Use the Data provided by your instructor.  


---

## ğŸš€ Step 5: Create a Storage Account  
ğŸ’¡ **Why?** This **Storage Account** will be used for **future data transfers from Fabric**.  

### 1ï¸âƒ£ Create an Azure Storage Account  
ğŸ”¹ Using the **Azure Portal**, create a **Storage Account**.  

ğŸ”¹ **What configuration settings are important for security and performance?**  

âœ… **Outcome**: Your **Storage Account** is now ready for future **data movement**.  

---

## ğŸš€ Step 6: Create & Configure a Service Principal  
ğŸ’¡ **Why?** Azure services require **authentication** to **access and transfer data securely**.  

### 1ï¸âƒ£ Create a Service Principal in Azure Entra ID  
ğŸ”¹ In **Azure Entra ID (Active Directory)**, register a **new application** for the **Service Principal**.  

ğŸ”¹ **What two key pieces of information do you need after creating the Service Principal?**  

âœ… **Outcome**: Your **Service Principal** is registered, but it still needs **permissions**.  

---

## ğŸš€ Step 7: Generate & Store Service Principal Credentials  
ğŸ’¡ **Why?** The **Service Principal** needs **credentials** to authenticate.  

### 1ï¸âƒ£ Create a Client Secret  
ğŸ”¹ Generate a **Client Secret** and **securely store** its value.  

ğŸ”¹ **Why is it important to save the secret value immediately?**  

âœ… **Outcome**: You now have the **Client ID, Tenant ID, and Secret Key** for authentication.  

---

## ğŸš€ Step 8: Assign Permissions in Fabric & Blob Storage  
ğŸ’¡ **Why?** The **Service Principal** must be granted **access** to Fabric and Azure Storage.  

### 1ï¸âƒ£ Enable Service Principal Authentication in Fabric  
ğŸ”¹ In **Microsoft Fabric**, enable **Service Principal authentication** for **Fabric APIs**.  

ğŸ”¹ **What Fabric Admin Setting must be changed to allow API access?**  

âœ… **Outcome**: The **Service Principal** can now **authenticate with Fabric APIs**.  

### 2ï¸âƒ£ Assign Permissions in Storage Account  
ğŸ”¹ Go to the **Storage Account** â†’ **Access Control (IAM)**  

ğŸ”¹ Assign the following **roles** to the **Service Principal**:  
   âœ… **Storage Blob Data Contributor**  
   âœ… **Storage Account Contributor**  
   âœ… **Cognitive Services Contributor** (*For Document Intelligence*)  

ğŸ”¹ **Why are these permissions necessary for the next steps of the challenge?**  

âœ… **Outcome**: The **Service Principal now has full access** to **Fabric & Blob Storage**.  

---

## ğŸš€ Step 9: Create a Document Intelligence Service in Azure  
ğŸ’¡ **Why?** The **Service** will analyze the **raw data**.  

---

## ğŸš€ Step 10: Create an Standard LRS Storage Account
ğŸ’¡ **Why?** The **Service** store you **analyzed JSON data**.  

---

## ğŸ Final Challenge Checkpoints  
âœ… Can you verify that **Fabric Capacity** is assigned correctly?  
âœ… Do all **financial PDFs** appear in **OneLake**?  
âœ… Does your **Two Storage Accounts, and Document Intelligence service** exist and are ready for use?  
âœ… Is your **Service Principal configured** and has necessary **permissions**?  

Once all steps are completed, you are ready to move on to **Challenge 2! ğŸš€**  

---

## â“ Troubleshooting Tips  
ğŸ”¹ If **Fabric** does not recognize your **Service Principal**, check if **API access** is enabled.  
ğŸ”¹ If you cannot **upload files to OneLake**, verify the assigned **capacity and workspace settings**.  
ğŸ”¹ If the **Storage Account** is not accessible, ensure the correct **RBAC roles** are assigned